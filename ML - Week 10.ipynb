{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Large Scale Machine Learning\n",
    "\n",
    "##### Stochastic Gradient Descent\n",
    "\n",
    "Performs cost function on a single example. It works by:\n",
    "\n",
    "1. Randomly Shuffle the dataset\n",
    "2. Repeat {\n",
    "for i = 1....,m {\n",
    "    $\\Theta_j := \\Theta_j - \\alpha (h_{\\Theta}(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}_j$\n",
    "    \n",
    "\n",
    "This algorithm will only to fit one training example at a time. This way it can make progress in gradient descent without having to scan all m training examples.\n",
    "\n",
    "It's unlikely to converge at a global minimum and will instead 'wander around it' randomly, but usually yields a result that is close enough. Stochastic gradient descent will usually take 1-10 passes through your dataset to get near the global minimum.\n",
    "\n",
    "\n",
    "##### Mini-batch Gradient Descent\n",
    "\n",
    "Uses _b_ examples in each iteration. Where _b_ is a parameter called the 'mini batch size'. Instead of using all m examples as in batch gradient descentm we use some in-between number of examples b. Typical values for b range from 2-100 or so.\n",
    "\n",
    "For example, with b=10 and m=1000:\n",
    "\n",
    "Repeat:\n",
    "\n",
    "for _i_ = 1,11,21,31,...,991\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha \\dfrac{1}{10} \\displaystyle \\sum_{k=i}^{i+9} (h_\\theta(x^{(k)}) - y^{(k)})x_j^{(k)}$\n",
    "\n",
    "This means that in this example we're running over 10 samples at a time. \n",
    "\n",
    "An advantage of computing more than one example at a time is that we can use vectorized implementations over b examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Online Learning\n",
    "\n",
    "With a continuous stream of users to a website, we can run an endless loop that gets (x,y), where we collect some user actions for the features in x to predict behaviour y.\n",
    "\n",
    "You can update $\\Theta$ for each individual (x,y) pair as you collect them. This way, you can adapt to new pools of users, since you're continously updating theta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map Reduce\n",
    "\n",
    "We can divide up batch gradient descent and dispatch the cost function for a subset of the data to many different machines so tha twe can train our algorithm in parallel.\n",
    "\n",
    "You can split your training set into z subsets corresponding to the number of machines being used. On each of those machines calculate $\\displaystyle \\sum_{i=p}^{q}(h_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}$ , where we've split the data starting at p and ending at q.\n",
    "\n",
    "MapReduce will take all these dispatched - or mapped - jobs and 'reduce'them by calculating:\n",
    "\n",
    "$\\Theta_j := \\Theta_j - \\alpha \\dfrac{1}{z}(temp_j^{(1)} + temp_j^{(2)} + \\cdots + temp_j^{(z)})$\n",
    "\n",
    "This is simply taking the computed cost from all the machines, calculating their average, multiplying by the learning rate, and updating theta.\n",
    "\n",
    "Your learning algorithm is MapReduceable if it can be expressed as computing sums of functions over the training set. Linear regression and logistic regression are easily parallelizable.\n",
    "\n",
    "For neural networks, you can compute forward propagation and back propagation on subsets of your data on many machines. Those machines can report their derivatives back to a 'master' server that will combine them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
