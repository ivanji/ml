{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Week 1: Linear Regression\n",
    "\n",
    "In supervised learning we know (sometimes only approximately) the values for _y_, for the _m_ samples in the training set $\\Xi$. We assume that if we can find a hypothesis _h_, that closely agrees with _y_ for the members of $\\Xi$, then this _h_ will be a good guess for _y_,specially if $\\Xi$ is large.\n",
    "\n",
    "$$ \\Xi = [{x_1, x_2,...x_i,...x_m}]$$\n",
    "\n",
    "$$h_\\theta(x) = x_0 + x_1 * x $$\n",
    "\n",
    "The above equation is a linear function and it's equivalent to the most widely known:\n",
    "\n",
    "$$ y = mx + b $$\n",
    "\n",
    "Another way to present this formula:\n",
    "$$\\hat{y} =  h_\\theta(x) = \\theta_0 + \\theta_1 * x $$\n",
    "\n",
    "$\\theta_0$ tells where the straight line intercepts with the _Y_ axis. \n",
    "$\\theta_1$ specifies the slope of the line or how steep it is. In other words, we're trying to create a function called $h_\\theta(x)$ that is trying to map our input data (x's) to our output data (y's).\n",
    "\n",
    "*Intuition*: \n",
    "- This equation simply means: $\\hat{y} = [a number] + [another number] * x$ // where _x_ is the variable we've been given.\n",
    "- $\\theta_0$ and $\\theta_1$ are normally guesses based on the training data. They're initiated using 0 or a very low value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "Suppose we have the following set of training data:\n",
    "\n",
    "| x | y |\n",
    "|-------|\n",
    "| 0 | 4 |\n",
    "| 1 | 7 |\n",
    "| 2 | 7 |\n",
    "| 3 | 8 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdac78ddef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training set = m\n",
    "x = [0,1,2,3] # e.g. Size \n",
    "y = [4,7,7,8] # e.g. Price\n",
    "\n",
    "fit = np.polyfit(x,y,1)\n",
    "fit_fn = np.poly1d(fit) \n",
    "# fit_fn is now a function which takes in x and returns an estimate for y\n",
    "\n",
    "plt.plot(x,y, 'yo', x, fit_fn(x), '--k')\n",
    "plt.xlim(0, 5)\n",
    "plt.ylim(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function (Squared Error Function)\n",
    "\n",
    "We measure the accuracy of our hypothesis function by using a *cost function*. This takes an average of all the results of the hypothesis with input from x's compared to the actual output y's.\n",
    "\n",
    "The formula used for this is called 'Sum of Squared Error function' (SSE) or 'Mean squared error function' (MSE).\n",
    "\n",
    "$$ J(\\theta_0, \\theta_1) = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left ( \\hat{y}_{i}- y_{i} \\right)^2  = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x_{i}) - y_{i} \\right)^2 $$\n",
    "\n",
    "Our objective is to get the best possible line in our hypothesis. The best possible line will be such so that the average squared vertical distances of the scattered points from the line will be the least. In other words, the closer the line is to the points in the training set, the miminum the errors, the more accurate our hypothesis is.\n",
    "\n",
    "From Michael Nielsen's Book:\n",
    "> Inspecting the form of the quadratic cost function, we see that C(w,b) is non-negative, since every term in the sum is non-negative. Furthermore, the cost C(w,b) becomes small, i.e., C(w,b)≈0, precisely when y(x) is approximately equal to the output, a, for all training inputs, x. So our training algorithm has done a good job if it can find weights and biases so that C(w,b)≈0.\n",
    "\n",
    "In the best case, the line should pass through all the points in our training data set. In such a case the value of $J(\\theta_0 , \\theta_1) $ will be 0.\n",
    "\n",
    "Then to minimise the value produced by our cost function, we use an algorithm called Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "We start with a guess for our hypothesis and then repeatedly apply gradient descent. Using this process are hypotesis will become more and more accurate.\n",
    "\n",
    "The gradient descent algorithm is:\n",
    "\n",
    "repeat until convergence:\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)$$\n",
    "\n",
    "When speficifically applied to the case of linear regression, a new gradient descent equation is used.\n",
    "\n",
    "\\begin{align*}\n",
    "  \\text{repeat until convergence: } \\lbrace & \\newline \n",
    "  \\theta_0 := & \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}(h_\\theta(x_{i}) - y_{i}) \\newline\n",
    "  \\theta_1 := & \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}\\left((h_\\theta(x_{i}) - y_{i}) x_{i}\\right) \\newline\n",
    "  \\rbrace&\n",
    "  \\end{align*}\n",
    "\n",
    "\n",
    "where *_m_* is the size of the training set, $\\theta_0$ a constant that will be changing simultaneously with $\\theta_1$ and $x_i,y_i$ are values of the given training set (data).\n",
    "\n",
    "Note that we have separated out the two cases for $\\theta_j$ into separate equations for $\\theta_0$ and $\\theta_1$; and that for $\\theta_1$ we are multiplying $x_i$ at the end due to the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We're formulating predictions based on existing data (training set). For each new input variable (x) a hypothesis is generated $\\hat{y}$. The difference in value between our prediction and the real value is called its cost or error. We generate an average of all these differences using a cost function (SSE), which generates a single output, the cost. (_note_: The differences in the values are squared in order to ensure we get a positive integer).\n",
    "\n",
    "To ensure our prediction is as accurate as possible we run the cost through another algorithm called '_Gradient Descent_'. (_note_: Gradient descent is a general algorithm that can be used to minimise values in other type of problems as well).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
