{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Terms\n",
    "\n",
    "_**Cost function**_: Function used to calculate the average of the error between our prediction and actual values. Synonyms with: _Loss_ or _Objective_ function.\n",
    "\n",
    "_**Error**_: A value representing the difference between predicted and actual value (from the training set). Low error value, more accurate our prediction. _Gradient Descent_ helps in minimizing the errors within our module.\n",
    "\n",
    "_**Feedforward**_: (Feedforward nueral network)  Where the output of one layer is used as the input to the next layer. Information is always fed forward, never fed back. Opposed to _Recurrent neural Networks_.\n",
    "\n",
    "_**Gradient Descent**_ : (batch) A very general (useful/popular) algorithm to reduce the error value produced by our cost function. It iterates through the entire dataset.\n",
    "\n",
    "_**Latent Features**_: These are features that are hidden within the available data. Not directly observable. PCA could be used to find latest features.\n",
    "\n",
    "_**Loss Function**_: See _Cost Function_.\n",
    "\n",
    "_**Map-Reduce**_: An approach in which the dataset is split into N numbers. N corresponds to a number of machines that will compute the cost of each subset in parallel.\n",
    "\n",
    "_**Means Squared Error**_: Cost function used to calculate error.\n",
    "\n",
    "_**Principal Component Analysis**\n",
    "\n",
    "_**Quadratic cost function**_: Also sometimes known means squared error (MSE). It's a cost function.\n",
    "\n",
    "_**Recurrent**_: Neural networks that allow loops within its layers. Neurons can fire a little while later for some limited duration of time, which in turn can then fire other neurons.\n",
    "\n",
    "_**Sliding Windows**_: Classifier used for object detection, which localises exactly where an object is in an image.\n",
    "\n",
    "_**Step Size**_: (Slide parameter), parameter used to define the amount to shift the rectangle used to localise objects in sliding windows classifier.\n",
    "\n",
    "_**Stochastic Gradient Descent**_: A cost function to measure how well the hypothesis is by performing this operation on a single example _x(i), y(i)_ on each iteration.\n",
    "\n",
    "#### Term Comparison\n",
    "\n",
    "| ML | Statistics |\n",
    "|-----------------|\n",
    "| Network, graphs | Model |\n",
    "| Weights | Parameters |\n",
    "| Learning | Fitting |\n",
    "| Supervised Learning | Regression/Classification |\n",
    "| Unsupervised learning | density estimation/clustering |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Sigmoid Function\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}.\n",
    "\\tag{3}\\end{eqnarray}\n",
    "\n",
    "$\\sigma $ is sometimes called _logistic function_, and neurons using them, _logistic neurons_\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
