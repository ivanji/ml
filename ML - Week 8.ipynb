{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning involves working with an unlabeled training set, where the algorithm is tasked with making sense of the data and creating clusters.\n",
    "\n",
    "Clustering is good for:\n",
    "- Market Segmentation\n",
    "- Social network analysis\n",
    "- Astronomical data analysis\n",
    "\n",
    "##### K-Means\n",
    "It's the most popular algorithm for automatically grouping data into coherent subsets (clusters). It works by:\n",
    "\n",
    "1. Initiliasing random data points known as cluster centroids.\n",
    "2. Assigning all examples into one of the groups, based on which cluster the example is closest to. This is known as clustering assignment.\n",
    "3. Move Centroid: computes the average for all the points inside each of the cluster centroid groups, then move the cluster centroid points to those averages.\n",
    "4. Re-run 2 and 3 until clusters have been found.\n",
    "\n",
    "Main variables are:\n",
    "\n",
    "- K (Number of clusters)\n",
    "- Training Set $X^{(1)}, X^{(2)}$ Where  $x^{(i)} \\in \\mathbb{R}^n$\n",
    "\n",
    "Repeat:\n",
    "\n",
    "```for i = 1 to m:\n",
    "      $c>{(i)}$:= index (from 1 to K) of cluster centroid closest to x^{(i)}\n",
    "    for k = 1 to K:\n",
    "       $\\mu_k$:= average (mean) of points assigned to cluster k```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Optimization Objective\n",
    "\n",
    "Notation:\n",
    "\n",
    "- $C^{(i)}$ = index of cluster (1,2,...*_K_*) to which example $x^{(i)}$ is currently assigned\n",
    "- $\\mu_k$ = cluster centroid _k_ $(\\mu_k \\in \\mathbb{R}^n$)\n",
    "- $\\mu_{c^i}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned\n",
    "\n",
    "Using the above variables we can define our *cost function*:\n",
    "\n",
    "$$ J(c^{(i)},\\dots,c^{(m)},\\mu_1,\\dots,\\mu_K) = \\dfrac{1}{m}\\sum_{i=1}^m ||x^{(i)} - \\mu_{c^{(i)}}||^2$$\n",
    "\n",
    "\n",
    "To randomly initialise K cluster centroids:\n",
    "\n",
    "- Should have K < m. e.g. k=2.\n",
    "- Randomly pick _K_ training examples.\n",
    "- Set $\\mu_1,...,\\mu_k$ equal to these _K_ examples.\n",
    "\n",
    "To avoid K-means getting stuck in local minima and not doing a good job at minimizing, we should try multiple random initialisations:\n",
    "\n",
    "`for 1 to 100 {\n",
    "    Randomly initialise k-means.\n",
    "    Run K-means. Get $c^{(1)},...,c^{(m)}, \\mu_1,...,\\mu_k.$\n",
    "    Compute cost function (distortion)\n",
    "    Pick clustering that gave lowest cost (distortion)\n",
    "}`\n",
    "\n",
    "This normally makes sense in clusterings between 2-10. For larger clusters running many random initialisations is normally not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the number of clusters\n",
    "\n",
    "What is right value of K? most likely ambigitious. \n",
    "\n",
    "Elbow method is one possible way of choosing the value K.\n",
    "\n",
    "##### Elbow Method:\n",
    "\n",
    "Start by initialising with a single cluster, then run cost function _J_. Then proceed to run K-means with 2 number of clusters, then 3 clusters and keep going until there's a low decline in the distortion.\n",
    "\n",
    "Choosing the value where there's a large deep in the distortion but from which distortion starts descending in a slower manner, is normally called the 'elbow'. As plotting this would resemble a human arm. Where the 'elbow' is then chosen as the possible ideal value of _K_\n",
    "\n",
    "More often than not, the results won't look like an elbow and ideal value won't be as obvious.\n",
    "\n",
    "Another more useful way of choosing the ideal value of _K_ depends on the actual purpose of the K-Means. Evaluating K-means based on a metric for how well it performs for that later purpose. In another words, choose by hand and then evaluate its performance based on the actual purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction\n",
    "\n",
    "This relates to Data compression, reducing the number of dimensions used to simplify the process of calculating results and at the same time improving performance.\n",
    "\n",
    "Make assumptions based on domain knowledge. If you know some dimensions are more important than others, you can just work with those.\n",
    "\n",
    "##### Principal Component Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is normally used to:\n",
    "    - To find latent (not directly observable) features driving patterns in data\n",
    "    - Reduce dimensionality\n",
    "        - Visualise high-dimensional data (reducing its dimensions. e.g. Reduce from 4D to 2D to scatter plot)\n",
    "        - Reduce noise in the data\n",
    "        - In preparation to run other algorithms that work better with fewer inputs\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
